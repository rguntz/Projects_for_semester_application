{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rguntz/Projects_for_semester_application/blob/main/Copie_de_Tutorial1_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=violet>A Simple Language Model</font>\n",
        "\n",
        "In this notebook we explore a very simple language model. We use the ```nltk``` library, a Python library for NLP. The goal is to get familiar with string probabilities and text generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "cpU9jfmAxWFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import bigrams, trigrams, WhitespaceTokenizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "id": "jxE14DEPAJ1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=violet>Corpus</font>\n",
        "\n",
        "We use a text from the **Gutenberg corpus**. The Gutenberg corpus is a collection of literary texts in ```nltk```."
      ],
      "metadata": {
        "id": "RETAn1LiyXPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the corpus\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "corpus = gutenberg.raw('carroll-alice.txt') # alice in wonderland text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p4qSSMzFKbw",
        "outputId": "67cc7f80-7b6a-4cdd-e12a-400f86c27359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus[:391])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw_ZhCEm6XwG",
        "outputId": "eef7d338-330d-4067-dac9-97e6bfd3c80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
            "\n",
            "CHAPTER I. Down the Rabbit-Hole\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\n",
            "book her sister was reading, but it had no pictures or conversations in\n",
            "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
            "conversation?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=violet>Tokenization</font>\n",
        "\n",
        "Tokenization **splits the text into units** (e.g., words). We tokenize with a whitespace tokenizer, which splits the input at whitespace characters (spaces, tabs, and newlines)."
      ],
      "metadata": {
        "id": "YD0zTr240add"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sample sentence to tokenize.\"\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sQ56msz0WRI",
        "outputId": "a794a7b1-bd8a-428b-91c5-6285b2ba2bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'sample', 'sentence', 'to', 'tokenize.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the lowered corpus\n",
        "raw_tokens = tokenizer.tokenize(corpus.lower())"
      ],
      "metadata": {
        "id": "MGbxXDo-FMtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually add EOS to the tokens that end with .?!\n",
        "# this \"separates\" sentences from each other\n",
        "tokens = []\n",
        "for token in raw_tokens:\n",
        "    tokens.append(token)\n",
        "    if token[-1] in {'?', '.', '!'}:\n",
        "        tokens.append('<EOS>')"
      ],
      "metadata": {
        "id": "tsJ9Iq1KZuFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=violet>String Probabilities with n-grams</font>\n",
        "\n",
        "We create a unigram model, a **frequency-based language model** that represents how often each individual word appears in the text. We do the same for bigrams (pairs of words) and trigrams."
      ],
      "metadata": {
        "id": "kQZhVtWy1EEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create unigrams, bigrams and trigrams\n",
        "unigram_model = FreqDist(tokens)\n",
        "bigram_model = FreqDist(bigrams(tokens))\n",
        "trigram_model = FreqDist(trigrams(tokens))"
      ],
      "metadata": {
        "id": "WuDbcZQY958x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 5 most common words in the unigram model:\")\n",
        "for word, count in unigram_model.most_common(5):\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi4_B8p3F6w5",
        "outputId": "93e84b6f-99a9-4458-f837-01baf5e42784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most common words in the unigram model:\n",
            "the: 1603\n",
            "<EOS>: 958\n",
            "and: 766\n",
            "to: 706\n",
            "a: 614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 5 most common bigrams:\")\n",
        "for bigram, count in bigram_model.most_common(5):\n",
        "    print(f\"{bigram}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcqN6I-xYSCQ",
        "outputId": "80364a26-901c-4da4-808b-045681fd8683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most common bigrams:\n",
            "('said', 'the'): 207\n",
            "('of', 'the'): 128\n",
            "('in', 'a'): 97\n",
            "('in', 'the'): 78\n",
            "('and', 'the'): 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 5 most common trigrams:\")\n",
        "for trigram, count in trigram_model.most_common(5):\n",
        "    print(f\"{trigram}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUJnLo6F-oC5",
        "outputId": "65d8322a-b050-45e8-c938-20523835cb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most common trigrams:\n",
            "('*', '*', '*'): 54\n",
            "('said', 'alice.', '<EOS>'): 33\n",
            "('the', 'mock', 'turtle'): 31\n",
            "('said', 'the', 'mock'): 19\n",
            "('she', 'said', 'to'): 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram_probability(word):\n",
        "    return unigram_model.freq(word) # frequency\n",
        "\n",
        "# conditional probability P(word | prev_word)\n",
        "def bigram_probability(prev_word, word):\n",
        "    if (prev_word == '<EOS>') or (prev_word not in unigram_model):\n",
        "        return 0\n",
        "    return bigram_model[prev_word, word] / unigram_model[prev_word]\n",
        "\n",
        "# conditional probability P(word | prev_word1, prev_word2)\n",
        "def trigram_probability(prev_word1, prev_word2, word):\n",
        "    if (prev_word2 == '<EOS>') or ((prev_word1, prev_word2) not in bigram_model):\n",
        "        return 0\n",
        "    return trigram_model[prev_word1, prev_word2, word] / bigram_model[(prev_word1, prev_word2)]"
      ],
      "metadata": {
        "id": "63fVOCfj-cdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# examples\n",
        "print(f\"Unigram Probability of 'alice': {unigram_probability('alice'):.4f}\")\n",
        "print(f\"Conditional probability p('said'|'alice'): {bigram_probability('alice', 'said'):.4f}\")\n",
        "print(f\"Conditional probability p('king'|'the'): {bigram_probability('the', 'king'):.4f}\")\n",
        "print(f\"Conditional probability p('said'|'the king'): {trigram_probability('the', 'king', 'said'):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9uBK6PbMLZk",
        "outputId": "e75dec7e-042d-4c99-d5e6-19455912b472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Probability of 'alice': 0.0081\n",
            "Conditional probability p('said'|'alice'): 0.0407\n",
            "Conditional probability p('king'|'the'): 0.0175\n",
            "Conditional probability p('said'|'the king'): 0.1786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define a function that computes the probability of multiple words forming (part of) a sentence based on the bigram model.\n",
        "\n",
        "$$\n",
        "p(w_1, w_2, \\dots, w_n) = p(w_1) \\prod_{i=2}^{n} p(w_i \\mid w_{i-1})\n",
        "$$\n",
        "\n",
        "Note that we use $p(w_1) = 1$ for simplicity."
      ],
      "metadata": {
        "id": "xF43UWl34b-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def string_probability(string, tokenizer): # with the bigrams\n",
        "    tokens = tokenizer.tokenize(string)\n",
        "    prob = 1.0\n",
        "    for i in range(len(tokens) - 1):\n",
        "        prob *= bigram_probability(tokens[i], tokens[i+1])\n",
        "    return prob"
      ],
      "metadata": {
        "id": "RDyJ1zpa4L4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_string = \"you are\"\n",
        "print(f\"Bigram Probability of '{input_string}': {string_probability(input_string, tokenizer):.6f}\")\n",
        "input_string = \"you are the\"\n",
        "print(f\"Bigram Probability of '{input_string}': {string_probability(input_string, tokenizer):.6f}\")\n",
        "input_string = \"you are the king\"\n",
        "print(f\"Bigram Probability of '{input_string}': {string_probability(input_string, tokenizer):.6f}\")\n",
        "\n",
        "# try a sentence that is wrong\n",
        "input_string = \"king the you\"\n",
        "print(f\"Bigram Probability of '{input_string}': {string_probability(input_string, tokenizer):.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuxI1foiSFWJ",
        "outputId": "d461b69c-175f-4259-a056-db07367c8467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Probability of 'you are': 0.011364\n",
            "Bigram Probability of 'you are the': 0.000284\n",
            "Bigram Probability of 'you are the king': 0.000005\n",
            "Bigram Probability of 'king the you': 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=violet>Text Generation</font>\n",
        "\n",
        "We generate text using a **bigram language model**. The model follows the bigram assumption and hence choses each word **depending** solely **on the previous word**. The model gets a starting word as input and iteratively selects the most probable next word based on bigram probabilities."
      ],
      "metadata": {
        "id": "PI6H9A_p36WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(starting_word, length=5):\n",
        "    generated_text = [starting_word]\n",
        "    current_word = starting_word\n",
        "\n",
        "    for _ in range(length - 1):\n",
        "        next_word = max(unigram_model, key = lambda word: bigram_probability(current_word, word))\n",
        "        generated_text.append(next_word)\n",
        "        current_word = next_word\n",
        "\n",
        "    return ' '.join(generated_text)"
      ],
      "metadata": {
        "id": "A0Pn_iHp18fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text:\", generate_text('you', length=6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5q7kN0oHTcS",
        "outputId": "f29f1055-a570-4d56-df4b-85a8a7033183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: you know what i don't know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text:\", generate_text('the', length=20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8Yuv7CEAjpE",
        "outputId": "09c46580-fb02-4947-b7ff-35ba6c0c3683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: the mock turtle in a little thing i don't know what i don't know what i don't know what i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: What is the fundamental issue of this model? *Hint:* Try to generate longer text."
      ],
      "metadata": {
        "id": "tWWpV3zz7Yjf"
      }
    }
  ]
}